{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os.path\n",
    "import networkx as nx\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import re\n",
    "import selfies as sf\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from rdkit import Chem\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Dict, List, Tuple\n",
    "from utils.chem_utils import ATOM_FDIM, BOND_FDIM, get_atom_features_sparse, get_bond_features\n",
    "from utils.rxn_graphs import RxnGraph\n",
    "from utils.data_utils import get_graph_features_from_smi, load_vocab, make_vocab, \\\n",
    "    tokenize_selfies_from_smiles, tokenize_smiles, S2SDataset, G2SDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ATOM_FDIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES: CC(C)[C@@H]C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START /usr/bin/eog \"/tmp/tmpg34kxo4x.PNG\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(eog:362832): Gtk-WARNING **: 22:39:36.874: cannot open display: localhost:15.0\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import time\n",
    "from rdkit.Chem import AllChem\n",
    "def DrawSMILES(smiles):\n",
    "    print(\"SMILES:\", smiles)\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    timestamp = time.time()\n",
    "    Draw.MolToFile(mol, \"./images/output\" + str(timestamp) + \".png\")\n",
    "    print(\"图片已保存为\" + str(timestamp) + \".png\")\n",
    "DrawSMILES(\"CC(C)[C@@H](N)C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 24],\n",
       "       [24, 17]], dtype=int32)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles = \"CC1=CC(=CC=C1)N(C=C(COC2=CC=C(C=C2)C(C)C)N)N.C1[C@@H]2[C@H]([C@@H]3[C@H]1C(=O)OC3=O)C(=O)OC2=O\"\n",
    "get_graph_features_from_smi((0,smiles,False))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.train_bin = \"/home/chenlidong/poly2SMILES/preprocessed/sample_g2s_series_rel_smiles_smiles/train_0.npz\"\n",
    "args.vocab_file = \"/home/chenlidong/poly2SMILES/preprocessed/sample_g2s_series_rel_smiles_smiles/vocab_smiles.txt\"\n",
    "args.verbose = True\n",
    "args.batch_type = \"tokens\"\n",
    "args.train_batch_size = 64\n",
    "args.enable_amp = False\n",
    "args.compute_graph_distance = True\n",
    "args.task = \"reaction_prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = G2SDataset(args, file=args.train_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['args', 'a_scopes', 'b_scopes', 'a_features', 'b_features', 'a_graphs', 'b_graphs', 'src_token_ids', 'src_lens', 'tgt_token_ids', 'tgt_lens', 'data_indices', 'batch_sizes', 'batch_starts', 'batch_ends', 'vocab', 'vocab_tokens', 'a_scopes_indices', 'b_scopes_indices', 'a_features_indices', 'b_features_indices', 'data_size'])\n"
     ]
    }
   ],
   "source": [
    "var = vars(train_dataset)\n",
    "print(var.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(var['batch_ends'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.sort()\n",
    "train_dataset.shuffle_in_bucket(bucket_size=1000)\n",
    "train_dataset.batch(\n",
    "            batch_type=args.batch_type,\n",
    "            batch_size=args.train_batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda _batch: _batch[0],\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, batch in enumerate(train_loader):\n",
    "#     print(batch.agraph)\n",
    "#     print(batch.fnode)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100]\n"
     ]
    }
   ],
   "source": [
    "var = vars(train_dataset)\n",
    "print(var['batch_ends'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_value(value, min_value, max_value):\n",
    "    scaled_value = (value - min_value) / (max_value - min_value) * 9.999\n",
    "    return scaled_value\n",
    "\n",
    "def unscale_value(scaled_value, min_value, max_value):\n",
    "    value = scaled_value / 9.999 * (max_value - min_value) + min_value\n",
    "    return value\n",
    "\n",
    "# 已知的最大值和最小值\n",
    "min_value = 0.6870349645614624\n",
    "max_value = 2.5119409561157227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "unscale_value(0.6652,min_value,max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def tokenize_smiles(smi: str) -> str:\n",
    "    pattern = r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    assert smi == \"\".join(tokens), f\"Tokenization mismatch. smi: {smi}, tokens: {tokens}\"\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def canonicalize_smiles(smiles, remove_atom_number=False, trim=True, suppress_warning=False):\n",
    "    cano_smiles = \"\"\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    if mol is None:\n",
    "        cano_smiles = \"\"\n",
    "\n",
    "    else:\n",
    "        if trim and mol.GetNumHeavyAtoms() < 2:\n",
    "            if not suppress_warning:\n",
    "                logging.info(f\"Problematic smiles: {smiles}, setting it to 'CC'\")\n",
    "            cano_smiles = \"CC\"          # TODO: hardcode to ignore\n",
    "        else:\n",
    "            if remove_atom_number:\n",
    "                [a.ClearProp('molAtomMapNumber') for a in mol.GetAtoms()]\n",
    "            cano_smiles = Chem.MolToSmiles(mol, isomericSmiles=True)\n",
    "\n",
    "    return cano_smiles\n",
    "\n",
    "def escape_special_characters(string):\n",
    "    escaped_string = string.replace('[', r'\\[').replace(']', r'\\]').replace('*', r'\\*')\n",
    "    return escaped_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "text = \"*CCCc1nccnc1CCCN1C(=O)[C@@H]2[C@@H](CC(C)=C3CO[C@H]4[C@H]5OCC6=C(C)C[C@@H]7C(=O)N(*)C(=O)[C@@H]7[C@@H]6[C@H]5C[C@H]4[C@@H]32)C1=O\"\n",
    "print(r\"{}\".format(tokenize_smiles(((text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"* C C C c 1 n c c n c 1 C C C N 1 C ( = O ) [C@@H] 2 [C@@H] ( C C ( C ) = C 3 C O [C@H] 4 [C@H] 5 O C C 6 = C ( C ) C [C@@H] 7 C ( = O ) N ( * ) C ( = O ) [C@@H] 7 [C@@H] 6 [C@H] 5 C [C@H] 4 [C@@H] 3 2 ) C 1 = O\"\n",
    "\"* C C C c 1 n c c n c 1 C C C N 1 C ( = O ) [C@H] 2 [C@@H] 3 C ( = C ( C ) C [C@H] 2 C 1 = O ) C O [C@@H] 1 [C@H] 3 C [C@H] 2 [C@@H] 1 O C C 1 = C ( C ) C [C@@H] 3 C ( = O ) N ( * ) C ( = O ) [C@@H] 3 [C@@H] 1 2\"\n",
    "\"* C C C c 1 n c c n c 1 C C C N 1 C ( = O ) [C@@H] 2 [C@@H] ( C C ( C ) = C 3 C O [C@H] 4 [C@H] 5 O C C 6 = C ( C ) C [C@@H] 7 C ( = O ) N ( * ) C ( = O ) [C@@H] 7 [C@@H] 6 [C@H] 5 C [C@H] 4 [C@@H] 3 2 ) C 1 = O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "smiles = \"*CCCc1nccnc1CCCN1C(=O)c2ccc3c4c(Oc5cc(C(C)(C)C)cc(C(C)(C)C)c5)cc5c6c(ccc(c7c(Oc8cc(C(C)(C)C)cc(C(C)(C)C)c8)cc(c2c37)C1=O)c64)C(=O)N(*)C5=O\"\n",
    "print(r\"{}\".format(escape_special_characters(tokenize_smiles(canonicalize_smiles(smiles)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def numEmbedding(num):\n",
    "    num = str(num)\n",
    "    regex = re.compile(r\"\\s*?(\\+|-)?(\\d+)(\\.)?(\\d+)?\\s*\")\n",
    "    tokens = []\n",
    "    matched = regex.match(num)\n",
    "    if matched:\n",
    "        sign, units, dot, decimals = matched.groups()\n",
    "        if sign:\n",
    "            tokens += [f\"_{sign}_\"]\n",
    "        tokens += [\n",
    "            f\"_{number}_{position}_\" for position, number in enumerate(units[::-1])\n",
    "        ][::-1]\n",
    "        if dot:\n",
    "            tokens += [f\"_{dot}_\"]\n",
    "        if decimals:\n",
    "            tokens += [\n",
    "                f\"_{number}_-{position}_\"\n",
    "                for position, number in enumerate(decimals, 1)\n",
    "            ]\n",
    "            \n",
    "    #for convinence this is to be modified\n",
    "    for i in range(len(tokens),6):\n",
    "        tokens += [\"_0_-\"+ str(i-1) +\"_\"]\n",
    "        \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "val1 = 1.0978717803955078\n",
    "scale_num = scale_value(val1,min_value,max_value)\n",
    "scale_num = round(scale_num, 4)\n",
    "scale_num_embedding = numEmbedding(scale_num)\n",
    "scale_num_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "# 指定文件夹路径\n",
    "folder_path = \"/home/chenlidong/data/800w_store/sample\"  # 替换为实际的文件夹路径\n",
    "\n",
    "\n",
    "global index_src\n",
    "global index_tgt\n",
    "index_src = 0\n",
    "index_tgt = 0\n",
    "def src(val1, val2):\n",
    "    global index_src\n",
    "    index_src+=1\n",
    "    print(\"src\" + str(index_src))\n",
    "    result = tokenize_smiles(canonicalize_smiles(val1)) + \" . \" + tokenize_smiles(canonicalize_smiles(val2))  # 示例：将两列相加\n",
    "    return result\n",
    "def tgt(val1, val2):\n",
    "    global index_tgt\n",
    "    index_tgt+=1\n",
    "    print(\"tgt\" + str(index_tgt))\n",
    "    scale_num = scale_value(val1,min_value,max_value)\n",
    "    scale_num = round(scale_num, 4)\n",
    "    scale_num_embedding = numEmbedding(scale_num)\n",
    "    result = scale_num_embedding + \" \" + tokenize_smiles(canonicalize_smiles(val2))\n",
    "    # result = tokenize_smiles(canonicalize_smiles(val2))\n",
    "    return result\n",
    "\n",
    "\n",
    "df_all = pd.DataFrame(index=None)\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):  # 确保文件是CSV文件\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # 读取CSV文件并进行处理\n",
    "        data = pd.read_csv(file_path, skiprows=1)\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        df_cur = pd.DataFrame(index=None)\n",
    "        df_cur[0] = data.apply(lambda row: src(row[8], row[9]), axis=1)\n",
    "        df_cur[1] = data.apply(lambda row: tgt(row[1], row[0]), axis=1)\n",
    "        df_all = pd.concat([df_all,df_cur])\n",
    "        \n",
    "        \n",
    "shuffled_df = df_all.sample(frac=1, random_state=12138)\n",
    "shuffled_df[0].to_csv(\"/home/chenlidong/data/800w_process/sample/src.csv\",index=False, header=False)\n",
    "shuffled_df[1].to_csv(\"/home/chenlidong/data/800w_process/sample/tgt.csv\",index=False, header=False)\n",
    "\n",
    "        # df_src = pd.DataFrame(index=None)\n",
    "        # df_tgt = pd.DataFrame(index=None)\n",
    "        # df_tgt = data.apply(lambda row: tgt(row[1], row[0]), axis=1)\n",
    "        # df_src = data.apply(lambda row: src(row[8], row[9]), axis=1)\n",
    "        # df_src.to_csv(\"/home/chenlidong/data/800w_process/sample/src.csv\",mode='a',index=False, header=False)\n",
    "        # df_tgt.to_csv(\"/home/chenlidong/data/800w_process/sample/tgt.csv\",mode='a',index=False, header=False)\n",
    "        # print(df_tgt)\n",
    "        # print(df_src)\n",
    "        # time.sleep(100)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/chenlidong/data/grap2smiles/train.txt\")\n",
    "\n",
    "def src(val1, val2):\n",
    "    # 在这里进行你的自定义操作\n",
    "    result = tokenize_smiles(canonicalize_smiles(val1)) + \" . \" + tokenize_smiles(canonicalize_smiles(val2))  # 示例：将两列相加\n",
    "    return result\n",
    "def tgt(val1, val2):\n",
    "    scale_num = scale_value(val1,min_value,max_value)\n",
    "    scale_num = round(scale_num, 4)\n",
    "    scale_num_embedding = numEmbedding(scale_num)\n",
    "    result = scale_num_embedding + \" \" + tokenize_smiles(canonicalize_smiles(val2))\n",
    "    # result = tokenize_smiles(canonicalize_smiles(val2))\n",
    "    return result\n",
    "\n",
    "\n",
    "df_src = pd.DataFrame(index=None)\n",
    "df_tgt = pd.DataFrame(index=None)\n",
    "df_tgt[1] = df.apply(lambda row: tgt(row[1], row[0]), axis=1)\n",
    "df_src[0] = df.apply(lambda row: src(row[8], row[9]), axis=1)\n",
    "df_src.to_csv(\"./data/Ma/test_src.csv\",index=False)\n",
    "df_tgt.to_csv(\"./data/Ma/train_tgt.csv\",index=False)\n",
    "# print(src(\"C1=CC(=NC=C1C2=CN=C3N2N=C(C=C3)N)N\",\"C1C[C@H]2[C@@H](C[C@@H]1[C@H]3CC[C@@H]4[C@H](C3)C(=O)OC4=O)C(=O)OC2=O\"))\n",
    "# print(tgt(1.2131241123123124,\"*c1ccc(-c2cnc3ccc(N4C(=O)[C@H]5CC[C@@H]([C@H]6CC[C@H]7C(=O)N(*)C(=O)[C@H]7C6)C[C@H]5C4=O)nn23)cn1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/chenlidong/data/grap2smiles/test.txt\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(df2.iloc[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "numEmbedding(\"1.11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_src = \"/home/chenlidong/Graph2SMILES/data/Ma/train_src.csv\"\n",
    "train_tgt = \"/home/chenlidong/Graph2SMILES/data/Ma/train_tgt.csv\"\n",
    "val_src = \"/home/chenlidong/Graph2SMILES/data/Ma/val_src.csv\"\n",
    "val_tgt = \"/home/chenlidong/Graph2SMILES/data/Ma/val_tgt.csv\"\n",
    "test_src = \"/home/chenlidong/Graph2SMILES/data/Ma/test_src.csv\"\n",
    "test_tgt = \"/home/chenlidong/Graph2SMILES/data/Ma/test_tgt.csv\"\n",
    "fns = {\n",
    "    \"train\": [(train_src, train_tgt)],\n",
    "    \"val\": [(val_src, val_tgt)],\n",
    "    \"test\": [(test_src, test_tgt)]\n",
    "}\n",
    "make_vocab(\n",
    "    fns=fns,\n",
    "    vocab_file=\"./vocab.txt\",\n",
    "    tokenized=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "names = ['*', 'C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'H', 'As', 'Al', 'I', 'B', 'Sb', 'Sn', 'Se', 'Ge', 'In', 'Pb', 'Te', 'Bi']\n",
    "frequencies = [8205087, 8205087, 8205087, 8205087, 1913804, 422611, 133356, 23862, 302380, 314574, 1358, 85165, 582, 97262, 2910, 2328, 2328, 10282, 1746, 194, 194, 582, 388]\n",
    "\n",
    "# 使用zip函数将两个列表合并为一个元组列表\n",
    "combined_list = list(zip(names, frequencies))\n",
    "\n",
    "# 使用sorted函数对元组列表进行排序，按照元组的第二个元素（频数）进行降序排序\n",
    "sorted_list = sorted(combined_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 获取排序后的名称列表\n",
    "sorted_names = [item[0] for item in sorted_list]\n",
    "sorted_freq = [item[1] for item in sorted_list]\n",
    "\n",
    "print(sorted_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Symbols for different atoms\n",
    "ATOM_LIST = ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe',\n",
    "             'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti',\n",
    "             'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr', 'Pt', 'Hg', 'Pb',\n",
    "             'W', 'Ru', 'Nb', 'Re', 'Te', 'Rh', 'Ta', 'Tc', 'Ba', 'Bi', 'Hf', 'Mo', 'U', 'Sm', 'Os', 'Ir',\n",
    "             'Ce', 'Gd', 'Ga', 'Cs', '*', 'unk']\n",
    "ATOM_DICT = {symbol: i for i, symbol in enumerate(ATOM_LIST)}\n",
    "\n",
    "MAX_NB = 10\n",
    "DEGREES = list(range(MAX_NB))\n",
    "HYBRIDIZATION = [Chem.rdchem.HybridizationType.SP,\n",
    "                 Chem.rdchem.HybridizationType.SP2,\n",
    "                 Chem.rdchem.HybridizationType.SP3,\n",
    "                 Chem.rdchem.HybridizationType.SP3D,\n",
    "                 Chem.rdchem.HybridizationType.SP3D2]\n",
    "HYBRIDIZATION_DICT = {hb: i for i, hb in enumerate(HYBRIDIZATION)}\n",
    "\n",
    "FORMAL_CHARGE = [-1, -2, 1, 2, 0]\n",
    "FC_DICT = {fc: i for i, fc in enumerate(FORMAL_CHARGE)}\n",
    "\n",
    "VALENCE = [0, 1, 2, 3, 4, 5, 6]\n",
    "VALENCE_DICT = {vl: i for i, vl in enumerate(VALENCE)}\n",
    "\n",
    "NUM_Hs = [0, 1, 3, 4, 5]\n",
    "NUM_Hs_DICT = {nH: i for i, nH in enumerate(NUM_Hs)}\n",
    "\n",
    "CHIRAL_TAG = [Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "              Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "              Chem.rdchem.ChiralType.CHI_UNSPECIFIED]\n",
    "CHIRAL_TAG_DICT = {ct: i for i, ct in enumerate(CHIRAL_TAG)}\n",
    "\n",
    "RS_TAG = [\"R\", \"S\", \"None\"]\n",
    "RS_TAG_DICT = {rs: i for i, rs in enumerate(RS_TAG)}\n",
    "\n",
    "BOND_TYPES = [None,\n",
    "              Chem.rdchem.BondType.SINGLE,\n",
    "              Chem.rdchem.BondType.DOUBLE,\n",
    "              Chem.rdchem.BondType.TRIPLE,\n",
    "              Chem.rdchem.BondType.AROMATIC]\n",
    "BOND_FLOAT_TO_TYPE = {\n",
    "    0.0: BOND_TYPES[0],\n",
    "    1.0: BOND_TYPES[1],\n",
    "    2.0: BOND_TYPES[2],\n",
    "    3.0: BOND_TYPES[3],\n",
    "    1.5: BOND_TYPES[4],\n",
    "}\n",
    "\n",
    "BOND_STEREO = [Chem.rdchem.BondStereo.STEREOE,\n",
    "               Chem.rdchem.BondStereo.STEREOZ,\n",
    "               Chem.rdchem.BondStereo.STEREONONE]\n",
    "\n",
    "BOND_DELTAS = {-3: 0, -2: 1, -1.5: 2, -1: 3, -0.5: 4, 0: 5, 0.5: 6, 1: 7, 1.5: 8, 2: 9, 3: 10}\n",
    "BOND_FLOATS = [0.0, 1.0, 2.0, 3.0, 1.5]\n",
    "\n",
    "RXN_CLASSES = list(range(10))\n",
    "\n",
    "# ATOM_FDIM = len(ATOM_LIST) + len(DEGREES) + len(FORMAL_CHARGE) + len(HYBRIDIZATION) \\\n",
    "#             + len(VALENCE) + len(NUM_Hs) + 1\n",
    "ATOM_FDIM = [len(ATOM_LIST), len(DEGREES), len(FORMAL_CHARGE), len(HYBRIDIZATION), len(VALENCE),\n",
    "             len(NUM_Hs), len(CHIRAL_TAG), len(RS_TAG), 2]\n",
    "# BOND_FDIM = 6\n",
    "BOND_FDIM = 9\n",
    "BINARY_FDIM = 5 + BOND_FDIM\n",
    "INVALID_BOND = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_features_sparse(atom: Chem.Atom, rxn_class: int = None, use_rxn_class: bool = False) -> List[int]:\n",
    "    \"\"\"Get atom features as sparse idx.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    atom: Chem.Atom,\n",
    "        Atom object from RDKit\n",
    "    rxn_class: int, None\n",
    "        Reaction class the molecule was part of\n",
    "    use_rxn_class: bool, default False,\n",
    "        Whether to use reaction class as additional input\n",
    "    \"\"\"\n",
    "    feature_array = []\n",
    "    symbol = atom.GetSymbol()\n",
    "    symbol_id = ATOM_DICT.get(symbol, ATOM_DICT[\"unk\"])\n",
    "    feature_array.append(symbol_id)\n",
    "\n",
    "    if symbol in [\"*\", \"unk\"]:\n",
    "        padding = [999999999] * len(ATOM_FDIM) if use_rxn_class else [999999999] * (len(ATOM_FDIM) - 1)\n",
    "        feature_array.extend(padding)\n",
    "\n",
    "    else:\n",
    "        degree_id = atom.GetDegree()\n",
    "        if degree_id not in DEGREES:\n",
    "            degree_id = 9\n",
    "        formal_charge_id = FC_DICT.get(atom.GetFormalCharge(), 4)\n",
    "        hybridization_id = HYBRIDIZATION_DICT.get(atom.GetHybridization(), 4)\n",
    "        valence_id = VALENCE_DICT.get(atom.GetTotalValence(), 6)\n",
    "        num_h_id = NUM_Hs_DICT.get(atom.GetTotalNumHs(), 4)\n",
    "        chiral_tag_id = CHIRAL_TAG_DICT.get(atom.GetChiralTag(), 2)\n",
    "\n",
    "        rs_tag = atom.GetPropsAsDict().get(\"_CIPCode\", \"None\")\n",
    "        rs_tag_id = RS_TAG_DICT.get(rs_tag, 2)\n",
    "\n",
    "        is_aromatic = int(atom.GetIsAromatic())\n",
    "        feature_array.extend([degree_id, formal_charge_id, hybridization_id,\n",
    "                              valence_id, num_h_id, chiral_tag_id, rs_tag_id, is_aromatic])\n",
    "\n",
    "        if use_rxn_class:\n",
    "            feature_array.append(rxn_class)\n",
    "\n",
    "    return feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63,\n",
       " 999999999,\n",
       " 999999999,\n",
       " 999999999,\n",
       " 999999999,\n",
       " 999999999,\n",
       " 999999999,\n",
       " 999999999,\n",
       " 999999999]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_atom_features_sparse(Chem.Atom(\"*\"), use_rxn_class=False, rxn_class=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27494df57cbed05ed7aa2fec09e0e9595e04b2c2431398925b22a976118b6520"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
